import os
import fitz  # PyMuPDF
import tempfile
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser 
from langchain_core.runnables import RunnablePassthrough

# Load Environmental Variables
load_dotenv()

# Gemini API KEY
google_api_key = os.getenv("GOOGLE_API_KEY") 

def pdf_processing(uploaded_pdfs):
    """
    Processes a list of uploaded PDF files from Streamlit and extracts text from each page.

    Args:
        uploaded_pdfs (list): List of uploaded PDF files.

    Returns:
        list: List of Document objects containing text and metadata.
    """
    pdf_text = []
    for pdf in uploaded_pdfs:
        pdf_name = pdf.name
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf.read())
            tmp_file_path = tmp_file.name
            
            doc = fitz.open(tmp_file_path) 
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text = page.get_text()
                document = Document(page_content=text, metadata={"source": pdf_name, "page_number": page_num + 1})
                pdf_text.append(document)
                
    return pdf_text

def text_chunking_uploading(pdf_text):
    """
    Chunks text from uploaded PDFs and uploads it to a vector database.
    
    Args:
        pdf_text (list): List of Document objects containing text and metadata.

    Returns:
        FAISS or None: The vector database or None if an error occurs.
    """
    if pdf_text: 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
        text_documents = text_splitter.split_documents(pdf_text)
        
        vector_db = FAISS.from_documents(text_documents, HuggingFaceBgeEmbeddings())
        vector_db.save_local("vector_store_of_recetly_uploaded_pdfs")
        vector_db
    else:
        None

# Loading the FAISS Index File 
vector_db = FAISS.load_local("vector_store_of_recetly_uploaded_pdfs", HuggingFaceBgeEmbeddings(), allow_dangerous_deserialization=True)


# Loading the gemini-1.5-flash Model
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.5,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)


template = """
You are a good chatbot developed by XYZ company.

Instructions:
1. Your job is to answer the question based on the provided context; don't go out of context to answer the question.
2. If the answer is not present in the context, respond with "I don't know the answer."

context:
{context}

question:
{question}

Answer:
"""

def chain(question):
    """
    Processes a question by chunking the PDF text, creating a vector database,
    and using a retriever to generate an answer through a LangChain RunnablePassthrough chain.

    Args:
        question (str): The question to be answered.
        vector_db (FAISS): The vector database used for retrieval.

    Returns:
        str: The answer generated by the chain.
    """
    try:

        retriever = vector_db.as_retriever()
        
        prompt = ChatPromptTemplate.from_template(template)
        
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        answer = chain.invoke(question) 
        
        return answer
    
    except Exception as e:
        return f"An error occurred during processing: {str(e)}"
